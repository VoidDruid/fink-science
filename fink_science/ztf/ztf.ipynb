{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Science modules in Fink: an example\n",
    "\n",
    "A science module contains necessary routines and classes to process the data, and add values. Typically, you will receive alerts in input, and output the same alerts with additional information. Input alert information contains position, flux, telescope properties, ... You can find what information is available in an alert [here](https://zwickytransientfacility.github.io/ztf-avro-alert/), or check the current [Fink added values](https://fink-broker.readthedocs.io/en/latest/science/added_values/).\n",
    "\n",
    "In this simple example, we explore a simple science module that takes magnitudes contained in each alert, and computes the change in magnitude between the last two measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility from fink-science\n",
    "from fink_science.utilities import concat_col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "Fink receives data as Avro. However, the internal processing makes use of Parquet files. We provide here alert data as Parquet: it contains original alert data from ZTF and some added values from Fink:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into a Spark DataFrame\n",
    "df = spark.read.format('parquet').load('sample.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check what's in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- candid: long (nullable = true)\n",
      " |-- schemavsn: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- objectId: string (nullable = true)\n",
      " |-- candidate: struct (nullable = true)\n",
      " |    |-- jd: double (nullable = true)\n",
      " |    |-- fid: integer (nullable = true)\n",
      " |    |-- pid: long (nullable = true)\n",
      " |    |-- diffmaglim: float (nullable = true)\n",
      " |    |-- pdiffimfilename: string (nullable = true)\n",
      " |    |-- programpi: string (nullable = true)\n",
      " |    |-- programid: integer (nullable = true)\n",
      " |    |-- candid: long (nullable = true)\n",
      " |    |-- isdiffpos: string (nullable = true)\n",
      " |    |-- tblid: long (nullable = true)\n",
      " |    |-- nid: integer (nullable = true)\n",
      " |    |-- rcid: integer (nullable = true)\n",
      " |    |-- field: integer (nullable = true)\n",
      " |    |-- xpos: float (nullable = true)\n",
      " |    |-- ypos: float (nullable = true)\n",
      " |    |-- ra: double (nullable = true)\n",
      " |    |-- dec: double (nullable = true)\n",
      " |    |-- magpsf: float (nullable = true)\n",
      " |    |-- sigmapsf: float (nullable = true)\n",
      " |    |-- chipsf: float (nullable = true)\n",
      " |    |-- magap: float (nullable = true)\n",
      " |    |-- sigmagap: float (nullable = true)\n",
      " |    |-- distnr: float (nullable = true)\n",
      " |    |-- magnr: float (nullable = true)\n",
      " |    |-- sigmagnr: float (nullable = true)\n",
      " |    |-- chinr: float (nullable = true)\n",
      " |    |-- sharpnr: float (nullable = true)\n",
      " |    |-- sky: float (nullable = true)\n",
      " |    |-- magdiff: float (nullable = true)\n",
      " |    |-- fwhm: float (nullable = true)\n",
      " |    |-- classtar: float (nullable = true)\n",
      " |    |-- mindtoedge: float (nullable = true)\n",
      " |    |-- magfromlim: float (nullable = true)\n",
      " |    |-- seeratio: float (nullable = true)\n",
      " |    |-- aimage: float (nullable = true)\n",
      " |    |-- bimage: float (nullable = true)\n",
      " |    |-- aimagerat: float (nullable = true)\n",
      " |    |-- bimagerat: float (nullable = true)\n",
      " |    |-- elong: float (nullable = true)\n",
      " |    |-- nneg: integer (nullable = true)\n",
      " |    |-- nbad: integer (nullable = true)\n",
      " |    |-- rb: float (nullable = true)\n",
      " |    |-- ssdistnr: float (nullable = true)\n",
      " |    |-- ssmagnr: float (nullable = true)\n",
      " |    |-- ssnamenr: string (nullable = true)\n",
      " |    |-- sumrat: float (nullable = true)\n",
      " |    |-- magapbig: float (nullable = true)\n",
      " |    |-- sigmagapbig: float (nullable = true)\n",
      " |    |-- ranr: double (nullable = true)\n",
      " |    |-- decnr: double (nullable = true)\n",
      " |    |-- sgmag1: float (nullable = true)\n",
      " |    |-- srmag1: float (nullable = true)\n",
      " |    |-- simag1: float (nullable = true)\n",
      " |    |-- szmag1: float (nullable = true)\n",
      " |    |-- sgscore1: float (nullable = true)\n",
      " |    |-- distpsnr1: float (nullable = true)\n",
      " |    |-- ndethist: integer (nullable = true)\n",
      " |    |-- ncovhist: integer (nullable = true)\n",
      " |    |-- jdstarthist: double (nullable = true)\n",
      " |    |-- jdendhist: double (nullable = true)\n",
      " |    |-- scorr: double (nullable = true)\n",
      " |    |-- tooflag: integer (nullable = true)\n",
      " |    |-- objectidps1: long (nullable = true)\n",
      " |    |-- objectidps2: long (nullable = true)\n",
      " |    |-- sgmag2: float (nullable = true)\n",
      " |    |-- srmag2: float (nullable = true)\n",
      " |    |-- simag2: float (nullable = true)\n",
      " |    |-- szmag2: float (nullable = true)\n",
      " |    |-- sgscore2: float (nullable = true)\n",
      " |    |-- distpsnr2: float (nullable = true)\n",
      " |    |-- objectidps3: long (nullable = true)\n",
      " |    |-- sgmag3: float (nullable = true)\n",
      " |    |-- srmag3: float (nullable = true)\n",
      " |    |-- simag3: float (nullable = true)\n",
      " |    |-- szmag3: float (nullable = true)\n",
      " |    |-- sgscore3: float (nullable = true)\n",
      " |    |-- distpsnr3: float (nullable = true)\n",
      " |    |-- nmtchps: integer (nullable = true)\n",
      " |    |-- rfid: long (nullable = true)\n",
      " |    |-- jdstartref: double (nullable = true)\n",
      " |    |-- jdendref: double (nullable = true)\n",
      " |    |-- nframesref: integer (nullable = true)\n",
      " |    |-- rbversion: string (nullable = true)\n",
      " |    |-- dsnrms: float (nullable = true)\n",
      " |    |-- ssnrms: float (nullable = true)\n",
      " |    |-- dsdiff: float (nullable = true)\n",
      " |    |-- magzpsci: float (nullable = true)\n",
      " |    |-- magzpsciunc: float (nullable = true)\n",
      " |    |-- magzpscirms: float (nullable = true)\n",
      " |    |-- nmatches: integer (nullable = true)\n",
      " |    |-- clrcoeff: float (nullable = true)\n",
      " |    |-- clrcounc: float (nullable = true)\n",
      " |    |-- zpclrcov: float (nullable = true)\n",
      " |    |-- zpmed: float (nullable = true)\n",
      " |    |-- clrmed: float (nullable = true)\n",
      " |    |-- clrrms: float (nullable = true)\n",
      " |    |-- neargaia: float (nullable = true)\n",
      " |    |-- neargaiabright: float (nullable = true)\n",
      " |    |-- maggaia: float (nullable = true)\n",
      " |    |-- maggaiabright: float (nullable = true)\n",
      " |    |-- exptime: float (nullable = true)\n",
      " |    |-- drb: float (nullable = true)\n",
      " |    |-- drbversion: string (nullable = true)\n",
      " |-- prv_candidates: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- jd: double (nullable = true)\n",
      " |    |    |-- fid: integer (nullable = true)\n",
      " |    |    |-- pid: long (nullable = true)\n",
      " |    |    |-- diffmaglim: float (nullable = true)\n",
      " |    |    |-- pdiffimfilename: string (nullable = true)\n",
      " |    |    |-- programpi: string (nullable = true)\n",
      " |    |    |-- programid: integer (nullable = true)\n",
      " |    |    |-- candid: long (nullable = true)\n",
      " |    |    |-- isdiffpos: string (nullable = true)\n",
      " |    |    |-- tblid: long (nullable = true)\n",
      " |    |    |-- nid: integer (nullable = true)\n",
      " |    |    |-- rcid: integer (nullable = true)\n",
      " |    |    |-- field: integer (nullable = true)\n",
      " |    |    |-- xpos: float (nullable = true)\n",
      " |    |    |-- ypos: float (nullable = true)\n",
      " |    |    |-- ra: double (nullable = true)\n",
      " |    |    |-- dec: double (nullable = true)\n",
      " |    |    |-- magpsf: float (nullable = true)\n",
      " |    |    |-- sigmapsf: float (nullable = true)\n",
      " |    |    |-- chipsf: float (nullable = true)\n",
      " |    |    |-- magap: float (nullable = true)\n",
      " |    |    |-- sigmagap: float (nullable = true)\n",
      " |    |    |-- distnr: float (nullable = true)\n",
      " |    |    |-- magnr: float (nullable = true)\n",
      " |    |    |-- sigmagnr: float (nullable = true)\n",
      " |    |    |-- chinr: float (nullable = true)\n",
      " |    |    |-- sharpnr: float (nullable = true)\n",
      " |    |    |-- sky: float (nullable = true)\n",
      " |    |    |-- magdiff: float (nullable = true)\n",
      " |    |    |-- fwhm: float (nullable = true)\n",
      " |    |    |-- classtar: float (nullable = true)\n",
      " |    |    |-- mindtoedge: float (nullable = true)\n",
      " |    |    |-- magfromlim: float (nullable = true)\n",
      " |    |    |-- seeratio: float (nullable = true)\n",
      " |    |    |-- aimage: float (nullable = true)\n",
      " |    |    |-- bimage: float (nullable = true)\n",
      " |    |    |-- aimagerat: float (nullable = true)\n",
      " |    |    |-- bimagerat: float (nullable = true)\n",
      " |    |    |-- elong: float (nullable = true)\n",
      " |    |    |-- nneg: integer (nullable = true)\n",
      " |    |    |-- nbad: integer (nullable = true)\n",
      " |    |    |-- rb: float (nullable = true)\n",
      " |    |    |-- ssdistnr: float (nullable = true)\n",
      " |    |    |-- ssmagnr: float (nullable = true)\n",
      " |    |    |-- ssnamenr: string (nullable = true)\n",
      " |    |    |-- sumrat: float (nullable = true)\n",
      " |    |    |-- magapbig: float (nullable = true)\n",
      " |    |    |-- sigmagapbig: float (nullable = true)\n",
      " |    |    |-- ranr: double (nullable = true)\n",
      " |    |    |-- decnr: double (nullable = true)\n",
      " |    |    |-- scorr: double (nullable = true)\n",
      " |    |    |-- magzpsci: float (nullable = true)\n",
      " |    |    |-- magzpsciunc: float (nullable = true)\n",
      " |    |    |-- magzpscirms: float (nullable = true)\n",
      " |    |    |-- clrcoeff: float (nullable = true)\n",
      " |    |    |-- clrcounc: float (nullable = true)\n",
      " |    |    |-- rbversion: string (nullable = true)\n",
      " |-- cutoutScience: struct (nullable = true)\n",
      " |    |-- fileName: string (nullable = true)\n",
      " |    |-- stampData: binary (nullable = true)\n",
      " |-- cutoutTemplate: struct (nullable = true)\n",
      " |    |-- fileName: string (nullable = true)\n",
      " |    |-- stampData: binary (nullable = true)\n",
      " |-- cutoutDifference: struct (nullable = true)\n",
      " |    |-- fileName: string (nullable = true)\n",
      " |    |-- stampData: binary (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- cdsxmatch: string (nullable = true)\n",
      " |-- rfscore: double (nullable = true)\n",
      " |-- snn_snia_vs_nonia: double (nullable = true)\n",
      " |-- snn_sn_vs_all: double (nullable = true)\n",
      " |-- mulens: struct (nullable = true)\n",
      " |    |-- class_1: string (nullable = true)\n",
      " |    |-- ml_score_1: double (nullable = true)\n",
      " |    |-- class_2: string (nullable = true)\n",
      " |    |-- ml_score_2: double (nullable = true)\n",
      " |-- roid: integer (nullable = true)\n",
      " |-- nalerthist: integer (nullable = true)\n",
      " |-- knscore: double (nullable = true)\n",
      " |-- fink_broker_version: string (nullable = true)\n",
      " |-- fink_science_version: string (nullable = true)\n",
      " |-- tracklet: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calling the science module\n",
    "\n",
    "First, you need to concatenate historical + current measurements for the quantities of interest. Here, we only need `magpsf`. Hence we create a new column to the DataFrame called `cmagpsf` (for _concatenated_ `magpsf`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cmagpsf', 'cjd', 'csigmapsf', 'cfid']"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Required alert columns\n",
    "what = ['magpsf', 'jd', 'sigmapsf', 'fid']\n",
    "\n",
    "# Use for creating temp name\n",
    "prefix = 'c'\n",
    "what_prefix = [prefix + i for i in what]\n",
    "\n",
    "# Concatenate historical + current measurements\n",
    "for colname in what:\n",
    "    df = concat_col(df, colname, prefix=prefix)\n",
    "\n",
    "what_prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import light_curve as lc\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_nans(arr):\n",
    "    if len(arr.shape)>1:\n",
    "        raise Exception(\"Only 1D arrays are supported.\")\n",
    "    \n",
    "    if np.isnan(arr[0]):\n",
    "        for elem in arr:\n",
    "            if not np.isnan(elem):\n",
    "                arr[0] = elem\n",
    "                break\n",
    "        else:\n",
    "            raise ValueError(\"nans only!\")\n",
    "\n",
    "    last_value = arr[0]\n",
    "    for idx, elem in enumerate(arr):\n",
    "        if not np.isnan(elem):\n",
    "            last_value = elem\n",
    "        else:\n",
    "            arr[idx] = last_value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_row(frame, index=0, dtype='float64'):\n",
    "    return np.array(frame[index][0], dtype=dtype)\n",
    "\n",
    "\n",
    "def npsave(*args, **kwargs):\n",
    "    args = [*args]\n",
    "    args[0] = f\"samples/{args[0]}\"\n",
    "    if not args[0].endswith('.npy'):\n",
    "        args[0] += '.npy'\n",
    "    return np.save(*args, **kwargs)\n",
    "\n",
    "\n",
    "def npload(*args, **kwargs):\n",
    "    args = [*args]\n",
    "    args[0] = f\"samples/{args[0]}\"\n",
    "    if not args[0].endswith('.npy'):\n",
    "        args[0] += '.npy'\n",
    "    args[0] = open(args[0], 'rb')\n",
    "    return np.load(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def create_sample(n=0):\n",
    "    cmagpsf = extract_row(df.select('cmagpsf').take(n+1), n)\n",
    "    cjd = extract_row(df.select('cjd').take(n+1), n)\n",
    "    csigmapsf = extract_row(df.select('csigmapsf').take(n+1), n)\n",
    "\n",
    "    if not os.path.exists(f'samples/{n}'):\n",
    "        os.mkdir(f'samples/{n}')\n",
    "\n",
    "    npsave(f\"{n}/cmagpsf\", cmagpsf)\n",
    "    npsave(f\"{n}/cjd\", cjd)\n",
    "    npsave(f\"{n}/csigmapsf\", csigmapsf)\n",
    "\n",
    "    return cmagpsf, cjd, csigmapsf\n",
    "\n",
    "\n",
    "def load_sample(n=0):\n",
    "    cmagpsf = npload(f\"{n}/cmagpsf\")\n",
    "    cjd = npload(f\"{n}/cjd\")\n",
    "    csigmapsf = npload(f\"{n}/csigmapsf\")\n",
    "\n",
    "    return cmagpsf, cjd, csigmapsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.npy', '0', '1', '4', '3', '2', '5']\n"
     ]
    }
   ],
   "source": [
    "create_sample(0)\n",
    "create_sample(1)\n",
    "create_sample(2)\n",
    "create_sample(3)\n",
    "create_sample(4)\n",
    "create_sample(5)\n",
    "\n",
    "print(os.listdir(\"samples\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(magpsf, jd, sigmapsf):\n",
    "    fix_nans(cmagpsf)\n",
    "    fix_nans(csigmapsf)\n",
    "\n",
    "    extractor = lc.Extractor(\n",
    "        lc.Amplitude(),\n",
    "        lc.BeyondNStd(nstd=1),\n",
    "        lc.LinearFit(),\n",
    "        lc.Mean(),\n",
    "        lc.Median(),\n",
    "        lc.StandardDeviation(),\n",
    "        lc.Cusum(),\n",
    "        lc.ExcessVariance(),\n",
    "        lc.MeanVariance(),\n",
    "        lc.Kurtosis(),\n",
    "        lc.MaximumSlope(),\n",
    "        lc.Skew(),\n",
    "        lc.WeightedMean(),\n",
    "        lc.Eta(),\n",
    "        lc.AndersonDarlingNormal(),\n",
    "        lc.ReducedChi2(),\n",
    "        lc.InterPercentileRange(quantile=0.1),\n",
    "        #lc.MagnitudePercentageRatio(),\n",
    "        lc.MedianBufferRangePercentage(quantile=0.1),\n",
    "        lc.PercentDifferenceMagnitudePercentile(quantile=0.1),\n",
    "        lc.MedianAbsoluteDeviation(),\n",
    "        lc.PercentAmplitude(),\n",
    "        lc.EtaE(),\n",
    "        lc.LinearTrend(),\n",
    "        lc.StetsonK(),\n",
    "        lc.WeightedMean(),\n",
    "        #lc.Bins(),\n",
    "        #lc.OtsuSplit(),\n",
    "    )\n",
    "\n",
    "    result = extractor(cjd, cmagpsf, csigmapsf)\n",
    "    print('\\n'.join(\"{} = {:.2f}\".format(name, value) for name, value in zip(extractor.names, result)))  # DEBUG\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> 34 nan nan\n",
      "<class 'numpy.ndarray'> 34 2459526.124098582 7.321780152225849\n",
      "<class 'numpy.ndarray'> 34 nan nan\n",
      "\n",
      "--------------\n",
      "\n",
      "amplitude = 1.52\n",
      "beyond_1_std = 0.47\n",
      "linear_fit_slope = -0.07\n",
      "linear_fit_slope_sigma = 0.00\n",
      "linear_fit_reduced_chi2 = 98.81\n",
      "mean = 18.47\n",
      "median = 18.22\n",
      "standard_deviation = 0.97\n",
      "cusum = 0.32\n",
      "excess_variance = 0.00\n",
      "mean_variance = 0.05\n",
      "kurtosis = -1.26\n",
      "maximum_slope = 123.08\n",
      "skew = -0.17\n",
      "weighted_mean = 17.55\n",
      "eta = 1.32\n",
      "anderson_darling_normal = 1.25\n",
      "chi2 = 112.38\n",
      "inter_percentile_range_10 = 2.52\n",
      "median_buffer_range_percentage_10 = 0.00\n",
      "percent_difference_magnitude_percentile_10 = 0.14\n",
      "median_absolute_deviation = 0.85\n",
      "percent_amplitude = 1.56\n",
      "eta_e = 591.79\n",
      "linear_trend = -0.07\n",
      "linear_trend_sigma = 0.02\n",
      "linear_trend_noise = 0.86\n",
      "stetson_K = 0.84\n",
      "weighted_mean = 17.55\n"
     ]
    }
   ],
   "source": [
    "cmagpsf, cjd, csigmapsf = load_sample(1)\n",
    "\n",
    "describe = lambda v: print(type(v), len(v), np.mean(v), np.std(v))\n",
    "\n",
    "assert cmagpsf.shape == cjd.shape == csigmapsf.shape, 'Mismatched shapes'\n",
    "\n",
    "describe(cmagpsf)\n",
    "describe(cjd)\n",
    "describe(csigmapsf)\n",
    "\n",
    "# separator for result\n",
    "print(\"\\n--------------\\n\")\n",
    "\n",
    "res = extract(cmagpsf, cjd, csigmapsf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 37:>                                                         (0 + 1) / 1]21/12/15 20:08:28 ERROR Executor: Exception in task 0.0 in stage 37.0 (TID 37)\n",
      "org.apache.spark.api.python.PythonException: TypeError: only size-1 arrays can be converted to Python scalars\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/serializers.py\", line 290, in dump_stream\n",
      "    for series in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 101, in <lambda>\n",
      "    return lambda *a: (verify_result_length(*a), arrow_return_type)\n",
      "  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 92, in verify_result_length\n",
      "    result = f(*a)\n",
      "  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/igor/Work/snad/fink-science/fink_science/ztf/processor.py\", line 22, in extract_features_ztf\n",
      "    jd_arr = jd.to_numpy().astype(float)\n",
      "ValueError: setting an array element with a sequence.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:172)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "21/12/15 20:08:28 WARN TaskSetManager: Lost task 0.0 in stage 37.0 (TID 37, localhost, executor driver): org.apache.spark.api.python.PythonException: TypeError: only size-1 arrays can be converted to Python scalars\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n",
      "    process()\n",
      "  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n",
      "    serializer.dump_stream(func(split_index, iterator), outfile)\n",
      "  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/serializers.py\", line 290, in dump_stream\n",
      "    for series in iterator:\n",
      "  File \"<string>\", line 1, in <lambda>\n",
      "  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 101, in <lambda>\n",
      "    return lambda *a: (verify_result_length(*a), arrow_return_type)\n",
      "  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 92, in verify_result_length\n",
      "    result = f(*a)\n",
      "  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/Users/igor/Work/snad/fink-science/fink_science/ztf/processor.py\", line 22, in extract_features_ztf\n",
      "    jd_arr = jd.to_numpy().astype(float)\n",
      "ValueError: setting an array element with a sequence.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:172)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:102)\n",
      "\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:100)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\n",
      "\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "21/12/15 20:08:28 ERROR TaskSetManager: Task 0 in stage 37.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1882.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 37.0 failed 1 times, most recent failure: Lost task 0.0 in stage 37.0 (TID 37, localhost, executor driver): org.apache.spark.api.python.PythonException: TypeError: only size-1 arrays can be converted to Python scalars\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/serializers.py\", line 290, in dump_stream\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 101, in <lambda>\n    return lambda *a: (verify_result_length(*a), arrow_return_type)\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 92, in verify_result_length\n    result = f(*a)\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/igor/Work/snad/fink-science/fink_science/ztf/processor.py\", line 22, in extract_features_ztf\n    jd_arr = jd.to_numpy().astype(float)\nValueError: setting an array element with a sequence.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:172)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:102)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:100)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor55.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: TypeError: only size-1 arrays can be converted to Python scalars\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/serializers.py\", line 290, in dump_stream\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 101, in <lambda>\n    return lambda *a: (verify_result_length(*a), arrow_return_type)\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 92, in verify_result_length\n    result = f(*a)\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/igor/Work/snad/fink-science/fink_science/ztf/processor.py\", line 22, in extract_features_ztf\n    jd_arr = jd.to_numpy().astype(float)\nValueError: setting an array element with a sequence.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:172)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:102)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:100)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0d/6k64288x2n7154c7n780k0j00000gn/T/ipykernel_54372/2629196360.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdf_change\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ztf_TEST'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextract_features_ztf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mwhat_prefix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf_change\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'objectId'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ztf_TEST'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Work/snad/spark-installation/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \"\"\"\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/snad/spark-installation/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/snad/spark-installation/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Work/snad/spark-installation/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1882.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 37.0 failed 1 times, most recent failure: Lost task 0.0 in stage 37.0 (TID 37, localhost, executor driver): org.apache.spark.api.python.PythonException: TypeError: only size-1 arrays can be converted to Python scalars\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/serializers.py\", line 290, in dump_stream\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 101, in <lambda>\n    return lambda *a: (verify_result_length(*a), arrow_return_type)\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 92, in verify_result_length\n    result = f(*a)\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/igor/Work/snad/fink-science/fink_science/ztf/processor.py\", line 22, in extract_features_ztf\n    jd_arr = jd.to_numpy().astype(float)\nValueError: setting an array element with a sequence.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:172)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:102)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:100)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:365)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3370)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$withAction(Dataset.scala:3369)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2550)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2764)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:254)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:291)\n\tat sun.reflect.GeneratedMethodAccessor55.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.api.python.PythonException: TypeError: only size-1 arrays can be converted to Python scalars\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 377, in main\n    process()\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 372, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/serializers.py\", line 290, in dump_stream\n    for series in iterator:\n  File \"<string>\", line 1, in <lambda>\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 101, in <lambda>\n    return lambda *a: (verify_result_length(*a), arrow_return_type)\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/worker.py\", line 92, in verify_result_length\n    result = f(*a)\n  File \"/Users/igor/Work/snad/spark-installation/python/lib/pyspark.zip/pyspark/util.py\", line 99, in wrapper\n    return f(*args, **kwargs)\n  File \"/Users/igor/Work/snad/fink-science/fink_science/ztf/processor.py\", line 22, in extract_features_ztf\n    jd_arr = jd.to_numpy().astype(float)\nValueError: setting an array element with a sequence.\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:456)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:172)\n\tat org.apache.spark.sql.execution.python.ArrowPythonRunner$$anon$1.read(ArrowPythonRunner.scala:122)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:410)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec$$anon$2.<init>(ArrowEvalPythonExec.scala:102)\n\tat org.apache.spark.sql.execution.python.ArrowEvalPythonExec.evaluate(ArrowEvalPythonExec.scala:100)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:127)\n\tat org.apache.spark.sql.execution.python.EvalPythonExec$$anonfun$doExecute$1.apply(EvalPythonExec.scala:89)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "# user-defined function from the current folder\n",
    "import importlib\n",
    "import processor\n",
    "from processor import extract_features_ztf\n",
    "importlib.reload(processor)\n",
    "\n",
    "df_change = df.withColumn('ztf_TEST', extract_features_ztf(*what_prefix))\n",
    "df_change.select(['objectId', 'ztf_TEST']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply the science module, that is creating a new column to the DataFrame whose values are the change in magnitude between the last 2 measurements. All the user logic is contained in the routine `deltamaglatest` defined in `processor.py`. This routine is a user-defined function that encapsulates the necessary operations, and it can call functions from user-defined modules (here `mymodule.py`) or third-party libraries (e.g. `numpy`, `pandas`, etc). Note that the input arguments of `deltamaglatest` are column names of the DataFrame, and they are materialised as `pd.Series` inside the routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+--------------------+\n",
      "|    objectId|          cdsxmatch|            deltamag|\n",
      "+------------+-------------------+--------------------+\n",
      "|ZTF18abjrdau|             PulsV*|  0.1650867462158203|\n",
      "|ZTF18abmmrzp|               Star|                null|\n",
      "|ZTF19abjfoad|Candidate_LensSyste|                null|\n",
      "|ZTF18acmwkqr|               RGB*|                null|\n",
      "|ZTF21acqeepb|            Unknown|                null|\n",
      "|ZTF17aaanpdf|       PulsV*delSct|  1.3444271087646484|\n",
      "|ZTF18abadigg|            Cepheid|  0.2772483825683594|\n",
      "|ZTF19aawfxge|                AGN|   -0.25921630859375|\n",
      "|ZTF18aaxypzn|                MIR|                null|\n",
      "|ZTF18abtrvkm|                 SN|                null|\n",
      "|ZTF18acmwkqr|               RGB*|  0.5792255401611328|\n",
      "|ZTF18abjcxoj|                SG*|                null|\n",
      "|ZTF18aaxyyjv|         PulsV*bCep| -0.9435768127441406|\n",
      "|ZTF18abcvdid|             Pulsar|  -0.055511474609375|\n",
      "|ZTF17aaabqqd|                 V*|                null|\n",
      "|ZTF18aazuljr|         Symbiotic*|                null|\n",
      "|ZTF18abnxbyb|              BLLac|                null|\n",
      "|ZTF18acgdrsy|            brownD*|                null|\n",
      "|ZTF18aaqfhlj|        HotSubdwarf|-0.10297966003417969|\n",
      "|ZTF18aazuljr|            Unknown| -1.4648609161376953|\n",
      "+------------+-------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_change = df.withColumn('deltamag', deltamaglatest('cmagpsf'))\n",
    "\n",
    "# print the result for the 20 first alerts\n",
    "df_change.select(['objectId', 'cdsxmatch', 'deltamag']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also quickly check some statistics on this new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+\n",
      "|summary|           deltamag|\n",
      "+-------+-------------------+\n",
      "|  count|                176|\n",
      "|   mean|0.09352213686162775|\n",
      "| stddev| 0.9564824046920042|\n",
      "|    min| -2.828317642211914|\n",
      "|    max| 3.4397459030151367|\n",
      "+-------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_change.select('deltamag').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Et voilà! Of course, this science module is extremely simple - but the logic remains the same for more complex cases!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
